{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Language Classification using Machine learning \n",
    "\n",
    "We all have used machine learning for data mining task and number crunching but it will be interesting to use machine learning in classifying the given text or sentence into the languages like German,Spanish,French,English.\n",
    "\n",
    "So to build language classification we will first need to train the model with the text data so that it is able to predict the type of language for given text, I think most of the browsers like **Google chrome,Mozilla or Internet explorer** would be using trained model under the hood whenever a user is landing on a page which is different than english.\n",
    "\n",
    "Below code will help in getting the text data from wikipedia page and will store it into text file locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple python script to collect text paragraphs from various languages on the\n",
    "# same topic namely the Wikipedia encyclopedia itself\n",
    "\n",
    "import os\n",
    "try:\n",
    "    # Python 2 compat\n",
    "    from urllib2 import Request, build_opener\n",
    "except ImportError:\n",
    "    # Python 3\n",
    "    from urllib.request import Request, build_opener\n",
    "\n",
    "import lxml.html\n",
    "from lxml.etree import ElementTree\n",
    "import numpy as np\n",
    "\n",
    "#urls for all the differnt wikipedia page\n",
    "pages = {\n",
    "    u'ar': u'http://ar.wikipedia.org/wiki/%D9%88%D9%8A%D9%83%D9%8A%D8%A8%D9%8A%D8%AF%D9%8A%D8%A7',\n",
    "    u'de': u'http://de.wikipedia.org/wiki/Wikipedia',\n",
    "    u'en': u'http://en.wikipedia.org/wiki/Wikipedia',\n",
    "    u'es': u'http://es.wikipedia.org/wiki/Wikipedia',\n",
    "    u'fr': u'http://fr.wikipedia.org/wiki/Wikip%C3%A9dia',\n",
    "    u'it': u'http://it.wikipedia.org/wiki/Wikipedia',\n",
    "    u'ja': u'http://ja.wikipedia.org/wiki/Wikipedia',\n",
    "    u'nl': u'http://nl.wikipedia.org/wiki/Wikipedia',\n",
    "    u'pl': u'http://pl.wikipedia.org/wiki/Wikipedia',\n",
    "    u'pt': u'http://pt.wikipedia.org/wiki/Wikip%C3%A9dia',\n",
    "    u'ru': u'http://ru.wikipedia.org/wiki/%D0%92%D0%B8%D0%BA%D0%B8%D0%BF%D0%B5%D0%B4%D0%B8%D1%8F',\n",
    "#    u'zh': u'http://zh.wikipedia.org/wiki/Wikipedia',\n",
    "}\n",
    "\n",
    "html_folder = u'html'\n",
    "text_folder = u'paragraphs'\n",
    "short_text_folder = u'short_paragraphs'\n",
    "n_words_per_short_text = 5\n",
    "\n",
    "#if Html folder does not exist create one \n",
    "if not os.path.exists(html_folder):\n",
    "    os.makedirs(html_folder)\n",
    "\n",
    "for lang, page in pages.items():\n",
    "\n",
    "    text_lang_folder = os.path.join(text_folder, lang)\n",
    "    if not os.path.exists(text_lang_folder): #Create paragragh folder\n",
    "        os.makedirs(text_lang_folder)\n",
    "\n",
    "    short_text_lang_folder = os.path.join(short_text_folder, lang)\n",
    "    if not os.path.exists(short_text_lang_folder): #Create Short paragragh folder\n",
    "        os.makedirs(short_text_lang_folder)\n",
    "\n",
    "    opener = build_opener()\n",
    "    html_filename = os.path.join(html_folder, lang + '.html')\n",
    "    if not os.path.exists(html_filename):\n",
    "        print(\"Downloading %s\" % page)\n",
    "        request = Request(page)\n",
    "        # change the User Agent to avoid being blocked by Wikipedia\n",
    "        # downloading a couple of articles ones should not be abusive\n",
    "        request.add_header('User-Agent', 'OpenAnything/1.0')\n",
    "        html_content = opener.open(request).read()\n",
    "        open(html_filename, 'wb').write(html_content)\n",
    "\n",
    "    # decode the payload explicitly as UTF-8 since lxml is confused for some\n",
    "    # reason\n",
    "    html_content = open(html_filename).read()\n",
    "    if hasattr(html_content, 'decode'):\n",
    "        html_content = html_content.decode('utf-8')\n",
    "    tree = ElementTree(lxml.html.document_fromstring(html_content))\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for p in tree.findall('//p'):\n",
    "        content = p.text_content()\n",
    "        if len(content) < 100:\n",
    "            # skip paragraphs that are too short - probably too noisy and not\n",
    "            # representative of the actual language\n",
    "            continue\n",
    "\n",
    "        text_filename = os.path.join(text_lang_folder,\n",
    "                                     '%s_%04d.txt' % (lang, i))\n",
    "        print(\"Writing %s\" % text_filename)\n",
    "        open(text_filename, 'wb').write(content.encode('utf-8', 'ignore'))\n",
    "        i += 1\n",
    "\n",
    "        # split the paragraph into fake smaller paragraphs to make the\n",
    "        # problem harder e.g. more similar to tweets\n",
    "        if lang in ('zh', 'ja'):\n",
    "        # FIXME: whitespace tokenizing does not work on chinese and japanese\n",
    "            continue\n",
    "        words = content.split()\n",
    "        n_groups = len(words) / n_words_per_short_text\n",
    "        if n_groups < 1:\n",
    "            continue\n",
    "        groups = np.array_split(words, n_groups)\n",
    "\n",
    "        for group in groups:\n",
    "            small_content = u\" \".join(group)\n",
    "\n",
    "            short_text_filename = os.path.join(short_text_lang_folder,\n",
    "                                               '%s_%04d.txt' % (lang, j))\n",
    "            print(\"Writing %s\" % short_text_filename)\n",
    "            open(short_text_filename, 'wb').write(\n",
    "                small_content.encode('utf-8', 'ignore'))\n",
    "            j += 1\n",
    "            if j >= 1000:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data will be stored into local folders of html,paragraghs and short_paragraphs.Now we need to load the text data into the memory for classification task.\n",
    "\n",
    "We will be using the load_files function of sklearn.datasets which can read the **2-level hierarchy structure like \"/Content/Category/file1.txt....file50.txt\".** \n",
    "\n",
    "The function takes the path of the folder as an input to read the files and returns a dataset with data and target attributes.\n",
    "\n",
    "Note: If you are curious to find underline structure of the load_files() function then you can use this following code to extract its detail.\n",
    "\n",
    "**import inspect**\n",
    "\n",
    "**print inspect.getsource(load_files)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#folder path containing our text files is passed as an argument.\n",
    "\n",
    "data_folder=os.getcwd()+\"\\\\paragraphs\"\n",
    "dataset = load_files(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import required library functions for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is already loaded in the dataset variable we will split it into train and test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the dataset in training and test set:\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the feature out of text data. We will be using two well known technique \n",
    "+ n-gram \n",
    "+ tf-idf (term frequency-inverse document frequency)\n",
    "\n",
    "## Uderstanding n-gram with simple example\n",
    "n-gram model are used in text mining and natural language processing task.\n",
    "\n",
    "They can be used for creating uni-gram,bi-gram,tri-gram model which in-turn can be used as an input for supervised learning task.\n",
    "\n",
    "Let see the example if we have sentence as **\"I love python language\"** and if N=2 then ngrams would be:\n",
    "+ I love\n",
    "+ love python\n",
    "+ python language\n",
    "\n",
    "Above was the example of bigram model where you typically move one word forward.\n",
    "\n",
    "Now lets try again with N=3 so our output will be\n",
    "\n",
    "+ I love python\n",
    "+ love python language\n",
    "\n",
    "This was a trigram model with N=3.\n",
    "\n",
    "So next question in mind can be how many n-gram can be created for a given sentence k \n",
    "\n",
    "Let's look for answer\n",
    "\n",
    "N-gram = X-(N-1) where X=\"Number of words in the sentence\" \n",
    "\n",
    "so for our sentence X = 4(**\"I love python language\"**)\n",
    "\n",
    "N-gram = 4-(2-1) = 3  (if N=2)\n",
    "\n",
    "N-gram = 4-(3-1) = 2  (if N=3)\n",
    "\n",
    "## Understanding the tf-idf with simple example\n",
    "\n",
    "Tf-idf stands for _term frequency-inverse document frequency_, and the tf-idf weight is a weight often used in information retrieval and text mining.\n",
    "\n",
    "It is measure used to evaluate how important a word is to a document in a collection or corpus.The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
    "\n",
    "TF: TermFrequency,which measures how frequently a term occurs in a document.\n",
    "\n",
    "**TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).**\n",
    "\n",
    "IDF: Inverse-DocumentFrequncy,which measures how important a term is. \n",
    "\n",
    "**IDF(t) = log_e(Total number of documents / Number of documents with term t in it).**\n",
    "\n",
    "Example: \n",
    "\n",
    "Consider a document containing 100 words wherein cat appears 3 times. so the term-frequency would be\n",
    " **tf:(3/100) -> 0.03**\n",
    "\n",
    "Now assume we have 10 million documents and cat appears 1000 times of these document then our Inverse-document frequency will be\n",
    " **idf:log_e(10,000,000/1000) = 4**\n",
    "\n",
    "So tf-idf weight will be product of these two quantities = 0.03 * 4 = 0.12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's perform ngram and tf-idf in one line and store it into variable vectorizer\n",
    "\n",
    "vectorizer=TfidfVectorizer(ngram_range=(1,3),analyzer='char',use_idf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code ngram_range=(1,3) tells function to create trigram feature model and analyzer='char' tells it to perform trigram for characters instead of words.\n",
    "\n",
    "_Note: We are not performing stop word removal,stemming in this task_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now since the feature building is done we will be running a Nueral network model called **perceptron** on top of it to perform the clasification task for different language types without any hyper-parameter tuning for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf=Pipeline([('vec',vectorizer),('clf',Perceptron())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline** function takes transformation and model function and returns the final model to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vec', TfidfVectorizer(analyzer='char', binary=False, charset=None,\n",
       "        charset_error=None, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), norm=u..._iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=False,\n",
       "      verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(docs_train,y_train) #fit the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted=clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         ar       1.00      1.00      1.00        13\n",
      "         de       0.99      1.00      0.99        79\n",
      "         en       0.99      1.00      0.99        78\n",
      "         es       0.98      1.00      0.99        51\n",
      "         fr       1.00      1.00      1.00        64\n",
      "         it       1.00      0.98      0.99        41\n",
      "         ja       1.00      1.00      1.00        32\n",
      "         nl       1.00      1.00      1.00        21\n",
      "         pl       1.00      1.00      1.00        19\n",
      "         pt       1.00      0.98      0.99        43\n",
      "         ru       1.00      0.97      0.98        31\n",
      "\n",
      "avg / total       0.99      0.99      0.99       472\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report\n",
    "print(metrics.classification_report(y_test, y_predicted,\n",
    "                                    target_names=dataset.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see our model is accurately able to predict the language types based on average precision and recall having 0.99 % accuracy for precision and recall.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets predict the news sentences using our language prediction model.\n",
    "\n",
    "Following are languages which we will be predicting \n",
    "+ en-English\n",
    "+ fr-French\n",
    "+ de-German\n",
    "+ ru-Russian\n",
    "+ it-Itatlian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The language of \"This is a language detection test.\" is \"en\"\n",
      "The language of \"Ceci est un test de détection de la langue.\" is \"fr\"\n",
      "The language of \"Dies ist ein Test, um die Sprache zu erkennen.\" is \"de\"\n",
      "The language of \"[ˌwɪkiˈpiːdiə]) — свободная[3] общедоступная мультиязычная\" is \"ru\"\n",
      "The language of \"Wikipedia – wielojęzyczna encyklopedia internetowa\" is \"it\"\n"
     ]
    }
   ],
   "source": [
    "# Predict the result on some short new sentences:\n",
    "sentences = [\n",
    "    u'This is a language detection test.',\n",
    "    u'Ceci est un test de d\\xe9tection de la langue.',\n",
    "    u'Dies ist ein Test, um die Sprache zu erkennen.',\n",
    "    u'[ˌwɪkiˈpiːdiə]) — свободная[3] общедоступная мультиязычная',\n",
    "    u'Wikipedia – wielojęzyczna encyklopedia internetowa'\n",
    "]\n",
    "\n",
    "predicted = clf.predict(sentences)\n",
    "\n",
    "for s, p in zip(sentences, predicted):\n",
    "    print(u'The language of \"%s\" is \"%s\"' % (s, dataset.target_names[p]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
